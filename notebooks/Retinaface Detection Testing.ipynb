{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from Retinaface.get_model import get_model\n",
    "from Retinaface.layers.prior_box import PriorBox\n",
    "from Retinaface.utils.box_utils import decode, decode_landm\n",
    "from Retinaface.utils.py_cpu_nms import py_cpu_nms\n",
    "from Retinaface.config import cfg_mnet\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, linewidth=300, precision=2)\n",
    "np.set_printoptions(suppress=True, linewidth=300, precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: 5119, Fake: 22095\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open dataframe and set filename as index\n",
    "df = pd.read_pickle('master_dataframe_updated.pkl')\n",
    "df.set_index('file', inplace=True)\n",
    "\n",
    "# Get list of all numpy files with faces detected\n",
    "npy = list(Path('/media/mc/2TBNVMESSD/train_bboxes/').glob('*.npy'))\n",
    "\n",
    "# Get list of real and fake numpy files\n",
    "names = [i.with_suffix('.mp4').name for i in npy]\n",
    "real = []\n",
    "fake = []\n",
    "for name in names:\n",
    "    label = df.loc[name, 'label']\n",
    "    if label == 'REAL': real.append(name)\n",
    "    elif label == 'FAKE': fake.append(name)\n",
    "    else: print('Unknown label: {}')\n",
    "print(f'Real: {len(real)}, Fake: {len(fake)}')\n",
    "\n",
    "# Make dataframe of npy info\n",
    "npy_df = pd.DataFrame(list(zip(names, npy)), columns=['file', 'npy_file'])\n",
    "npy_df.set_index('file', inplace=True)\n",
    "for name in names:\n",
    "    npy_df.loc[name, 'label'] = df.loc[name, 'label']\n",
    "    data = np.load(npy_df.loc[name, 'npy_file'], allow_pickle=True)\n",
    "    npy_df.loc[name, 'num_dets'] = np.array([i.shape[0] for i in data]).mean()\n",
    "\n",
    "# Make dataframe organizing processed files\n",
    "proc_df = pd.DataFrame(list(zip(real, [[] for i in real])), columns=['real', 'fakes'])\n",
    "proc_df.set_index('real', inplace=True)\n",
    "for name in fake:\n",
    "    original = df.loc[name, 'original']\n",
    "    if original in proc_df.index:\n",
    "        proc_df.loc[original, 'fakes'].append(name)\n",
    "\n",
    "# Plot histogram of fakes per real\n",
    "num = [len(proc_df.iloc[i, 0]) for i in range(len(proc_df))]\n",
    "plt.hist(num);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n"
     ]
    }
   ],
   "source": [
    "# Get real videos with more than one detection\n",
    "files = npy_df[(npy_df['num_dets'] > 1.0) & (npy_df['label'] == 'REAL')].index\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/mc/dev/Retinaface/Retinaface/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n",
      "Finished loading model\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model = get_model()\n",
    "model.eval()\n",
    "print(\"Finished loading model\")\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(file, max_frames=None, max_dim=512):\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if max_frames is None: total_frames = num_frames\n",
    "    elif max_frames > num_frames: total_frames = num_frames\n",
    "    else: total_frames = max_frames\n",
    "    resize = 1\n",
    "    if max_dim is not None:\n",
    "        max_img = max(width, height)\n",
    "        if max_img > max_dim:\n",
    "            resize = max_dim / max_img\n",
    "    frames = []\n",
    "    imgs = []\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "        if resize != 1:\n",
    "            imgs.append(cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_AREA))\n",
    "    return np.stack(frames).astype(np.float32), np.stack(imgs).astype(np.float32), resize\n",
    "\n",
    "def preprocess_imgs(imgs):\n",
    "    imgs -= (104, 117, 123)\n",
    "    imgs = imgs.transpose(0, 3, 1, 2)\n",
    "    imgs = torch.from_numpy(imgs)\n",
    "    return imgs\n",
    "\n",
    "def detect_imgs(imgs, model, cfg, resize, conf_thresh=0.5, nms_threshold=0.4):\n",
    "    scale = torch.Tensor([imgs.shape[3], imgs.shape[2], imgs.shape[3], imgs.shape[2]])\n",
    "    imgs = imgs.to(device)\n",
    "    scale = scale.to(device)\n",
    "    all_loc, all_conf, all_landms = model(imgs)\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(imgs.shape[2:4]))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "\n",
    "    all_dets = []\n",
    "    for loc, conf, landms in zip(all_loc, all_conf, all_landms):\n",
    "\n",
    "        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        boxes = boxes * scale / resize\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "        landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        scale1 = torch.Tensor([imgs.shape[3], imgs.shape[2], imgs.shape[3], imgs.shape[2],\n",
    "                               imgs.shape[3], imgs.shape[2], imgs.shape[3], imgs.shape[2],\n",
    "                               imgs.shape[3], imgs.shape[2]])\n",
    "        scale1 = scale1.to(device)\n",
    "        landms = landms * scale1 / resize\n",
    "        landms = landms.cpu().numpy()\n",
    "\n",
    "        # Ignore low scores\n",
    "        inds = np.where(scores > conf_thresh)[0]\n",
    "        boxes = boxes[inds]\n",
    "        landms = landms[inds]\n",
    "        scores = scores[inds]\n",
    "\n",
    "        # Keep top-K before NMS\n",
    "        order = scores.argsort()[::-1]\n",
    "        boxes = boxes[order]\n",
    "        landms = landms[order]\n",
    "        scores = scores[order]\n",
    "\n",
    "        # NMS\n",
    "        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "        keep = py_cpu_nms(dets, nms_threshold)\n",
    "        dets = dets[keep, :]\n",
    "        landms = landms[keep]\n",
    "        dets = np.concatenate((dets, landms), axis=1)\n",
    "\n",
    "        all_dets.append(dets)\n",
    "\n",
    "    return all_dets\n",
    "\n",
    "def annotate_img(img, dets):\n",
    "    ann_img = img.copy()\n",
    "    for det in dets:\n",
    "        # bbox\n",
    "        cv2.rectangle(ann_img, (det[0], det[1]), (det[2], det[3]), (0, 0, 255), 5)\n",
    "        # landms\n",
    "        cv2.circle(ann_img, (det[5], det[6]), 5, (255, 255, 0), 5)\n",
    "        cv2.circle(ann_img, (det[7], det[8]), 5, (0, 255, 255), 5)\n",
    "        cv2.circle(ann_img, (det[9], det[10]), 5, (255, 0, 255), 5)\n",
    "        cv2.circle(ann_img, (det[11], det[12]), 5, (0, 255, 0), 5)\n",
    "        cv2.circle(ann_img, (det[13], det[14]), 5, (255, 0, 0), 5)\n",
    "    return ann_img\n",
    "\n",
    "def annotate_imgs(imgs, dets):\n",
    "    ann_imgs = []\n",
    "    for img, det in zip(imgs, dets):\n",
    "        ann_imgs.append(annotate_img(img, det))\n",
    "    return np.stack(ann_imgs)\n",
    "\n",
    "def imgs2mp4(imgs, out_file):\n",
    "    width, height = imgs.shape[2], imgs.shape[1]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(out_file, fourcc, 30, (width, height))\n",
    "    for img in imgs:\n",
    "        out.write(img)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mc/dev/dfdc_train/dfdc_train_part_25/aadqbokerz.mp4\n"
     ]
    }
   ],
   "source": [
    "in_file = 'aadqbokerz.mp4'\n",
    "in_path = df.loc[in_file].filepath\n",
    "print(in_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for getting video frames: 1.08s\n",
      "Time frames preprocessing: 0.12s\n",
      "Time for model inference: 0.41s\n",
      "Total time: 1.61s\n"
     ]
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "\n",
    "start = time.time()\n",
    "frames, imgs, resize = get_frames(in_path, max_frames=75, max_dim=512)\n",
    "elapsed = time.time() - start\n",
    "print(f'Time for getting video frames: {elapsed:.2f}s')\n",
    "\n",
    "start = time.time()\n",
    "imgs = preprocess_imgs(imgs)\n",
    "elapsed = time.time() - start\n",
    "print(f'Time frames preprocessing: {elapsed:.2f}s')\n",
    "\n",
    "start = time.time()\n",
    "dets = detect_imgs(imgs, model, cfg_mnet, resize, conf_thresh=0.5, nms_threshold=0.4)\n",
    "elapsed = time.time() - start\n",
    "print(f'Time for model inference: {elapsed:.2f}s')\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "print(f'Total time: {total_elapsed:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/media/mc/2TBNVMESSD/retinaface_dets/'\n",
    "for in_path in df.filepath.to_numpy()[:10]:\n",
    "    out_path = str(Path(out_dir) / Path(in_path).stem) + '.npy'\n",
    "    if os.path.exists(out_path): continue\n",
    "    frames, imgs, resize = get_frames(in_path, max_frames=75, max_dim=512)\n",
    "    imgs = preprocess_imgs(imgs)\n",
    "    dets = detect_imgs(imgs, model, cfg_mnet, resize, conf_thresh=0.5, nms_threshold=0.4)\n",
    "    np.save(out_path, dets, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = np.load('/media/mc/2TBNVMESSD/retinaface_dets/aadqbokerz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_clip = annotate_imgs(frames, dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2mp4(ann_clip.astype(np.uint8), '/home/mc/Desktop/test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = np.load(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
